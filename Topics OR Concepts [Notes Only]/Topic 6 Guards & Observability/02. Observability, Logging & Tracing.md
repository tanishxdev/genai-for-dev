

# **Topic  — Observability, Logging & Tracing for Gemini Agents**

This topic is extremely important for real-world production agents because **you cannot debug what you cannot see**.

We will keep your preferred pattern:

```js
// GOAL: ...
import ai from "../utils/geminiClient.js";

async function main() {
   ...
}
await main();
```

No emojis. Clean English. Clear code. Reusable structure.

---

# **Topic 6.1 — Observability, Logging & Tracing**

---

## **1. Goal**

Learn to:

* Track API latency
* Log input/output safely
* Record tool usage
* Capture errors
* Measure token usage
* Add a structured logging wrapper

This will make your agent:

* Debuggable
* Traceable
* Production-ready
* Easy to audit

---

# **2. Concept: What Is Observability?**

In Agents:

* **Input Observability** → What was sent to the model
* **Output Observability** → How the model responded
* **Performance Observability** → Latency, errors, retry count
* **Tool Observability** → Which tools the model called and with what input

These logs are **not for the user**.
These are **developer logs** for debugging and understanding model behavior.

---

# **3. Simple Logging Wrapper**

Create a utility you can reuse anywhere.

## **File:** `utils/logger.js`

```js
// -------------------------------------------------------------
// Basic logger with timestamps and consistent formatting
// -------------------------------------------------------------

export function logInfo(message, data = null) {
  console.log(`[INFO ${new Date().toISOString()}] ${message}`);
  if (data !== null) console.dir(data, { depth: 5 });
}

export function logError(message, error) {
  console.error(`[ERROR ${new Date().toISOString()}] ${message}`);
  console.error(error);
}

export function logStart(operation) {
  console.log(`\n=== START: ${operation} ===`);
}

export function logEnd(operation) {
  console.log(`=== END: ${operation} ===\n`);
}
```

---

# **4. Observability applied to a Model Call**

## **File:** `src/10a.observability-basic.js`

```js
// -------------------------------------------------------------
// GOAL: Add observability to Gemini calls
// -------------------------------------------------------------

import ai from "../utils/geminiClient.js";
import { logStart, logEnd, logInfo, logError } from "../utils/logger.js";

async function main() {
  logStart("Describe Model");

  const prompt = "Explain Generative AI in one line.";

  try {
    const t0 = Date.now();

    const response = await ai.models.generateContent({
      model: "gemini-2.5-flash",
      contents: prompt,
    });

    const t1 = Date.now();

    logInfo("Prompt", prompt);
    logInfo("Raw Response", response.text);
    logInfo("Latency (ms)", t1 - t0);
    logInfo("Model Version", response.modelVersion);
    logInfo("Usage Metadata", response.usageMetadata);
  } catch (err) {
    logError("Model call failed", err);
  }

  logEnd("Describe Model");
}

await main();
```

---

# **5. Output Example**

Your terminal will show:

```
=== START: Describe Model ===
[INFO ...] Prompt
"Explain Generative AI in one line."

[INFO ...] Raw Response
"Generative AI creates new text, images, and ideas by learning patterns from data."

[INFO ...] Latency (ms)
321

[INFO ...] Model Version
"gemini-2.5-flash-latest"

[INFO ...] Usage Metadata
{ inputTokens: 15, outputTokens: 21 }

=== END: Describe Model ===
```

Why this matters:

* You know exactly how fast Gemini replies
* You know how many tokens it consumed (cost prediction)
* You logged the prompt and response for reproducibility

---

# **6. Observability for Tool Usage**

Update your tool-calling agent to log:

* When a tool is called
* What input it received
* What it returned
* The latency

## **File:** `src/10b.observability-tools.js`

```js
// -------------------------------------------------------------
// GOAL: Trace tool calls with detailed logs
// -------------------------------------------------------------

import ai from "../utils/geminiClient.js";
import fetch from "node-fetch";
import { logStart, logEnd, logInfo, logError } from "../utils/logger.js";

// Tool implementation with added logging
async function getGithubProfile({ username }) {
  logStart(`Tool: getGithubProfile (${username})`);
  const t0 = Date.now();

  const res = await fetch(`https://api.github.com/users/${username}`);
  const data = await res.json();

  const t1 = Date.now();
  logInfo("GitHub API Latency (ms)", t1 - t0);
  logInfo("GitHub API Raw Data", data);

  logEnd(`Tool: getGithubProfile (${username})`);
  return data;
}

const tools = [
  {
    name: "getGithubProfile",
    description: "Fetch GitHub profile info.",
    parameters: {
      type: "object",
      properties: { username: { type: "string" } },
      required: ["username"],
    },
    function: getGithubProfile,
  }
];

async function main() {
  logStart("Tool Observability");

  const prompt = "Fetch the GitHub profile for 'torvalds'.";

  try {
    const res = await ai.models.generateContent({
      model: "gemini-2.5-flash",
      contents: prompt,
      tools,
    });

    logInfo("Final Model Response", res.text);
  } catch (err) {
    logError("Tool-call failure", err);
  }

  logEnd("Tool Observability");
}

await main();
```

---

# **7. Logs You Will See**

```
=== START: Tool: getGithubProfile (torvalds) ===
[INFO ...] GitHub API Latency (ms) 180
[INFO ...] GitHub API Raw Data { ...actual GitHub json... }
=== END: Tool: getGithubProfile (torvalds) ===

[INFO ...] Final Model Response
"Linus Torvalds has ... followers."

=== END: Tool Observability ===
```

This confirms:

* The tool was called
* GitHub API returned live data
* Agent used it correctly

---

# **8. Observability for Full Agent**

You will integrate:

* Input log
* Output log
* Tool logs
* Latency
* Error boundaries

into your `09.full-agent.js` soon.

---

# **9. Notes (Add to Topics folder)**

Create:

```
/Topics OR Concepts/Topic 6 Guards & Observability/
   └── 02.observability.md
```

Content:

```
# Topic 6.1 – Observability (Logging & Tracing)

- Log every model prompt and result
- Log latency (measure performance)
- Log tool calls and tool results
- Log errors with context
- Log token usage from usageMetadata
- Helps debug, audit, and scale agents
```

---