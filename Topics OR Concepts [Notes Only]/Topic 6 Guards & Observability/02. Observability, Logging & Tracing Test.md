**Add COMPLETE observability + debugging + latency profiling + token cost + failure tracing**
Just like enterprise GenAI systems.

I’ll show you:

1. **Enhanced logger** (request/response + truncation + safe logs)
2. **Wrapper around Gemini client** (`instrumentedClient`)
3. **Per-call metrics**
4. **Failure classification** (timeout, schema error, rate limit, network)
5. **Automatic log grouping**
6. **Pretty print for long model outputs**
7. **Reusable middleware-based call wrapper**

This will replace your current observability with something 10x more powerful.

---

# PART 1 — UPGRADED LOGGER

Replace your `logger.js` with this **enterprise-grade but simple** logger.

```js
// utils/logger.js

// Helper to truncate long model outputs for logging
const truncate = (str, max = 300) =>
  typeof str === "string" && str.length > max
    ? str.slice(0, max) + " ...[truncated]"
    : str;

export function logInfo(message, data = null) {
  console.log(`[INFO  ${new Date().toISOString()}] ${message}`);
  if (data !== null) console.dir(data, { depth: 5, colors: true });
}

export function logWarn(message, data = null) {
  console.warn(`[WARN  ${new Date().toISOString()}] ${message}`);
  if (data !== null) console.dir(data, { depth: 5, colors: true });
}

export function logError(message, error) {
  console.error(`[ERROR ${new Date().toISOString()}] ${message}`);

  if (error instanceof Error) {
    console.error("Message:", error.message);
    console.error("Stack:", error.stack?.split("\n").slice(0, 5).join("\n"));
  } else {
    console.error(error);
  }
}

export function logStart(operation, meta = {}) {
  console.log(`\n=== START: ${operation} ===`);
  if (Object.keys(meta).length > 0) console.dir(meta, { depth: 5 });
}

export function logEnd(operation) {
  console.log(`=== END:   ${operation} ===\n`);
}

export { truncate };
```

This adds:

* Truncation
* Stack preview
* Warn logger
* Metadata on start

---

# PART 2 — WRAP GEMINI CLIENT WITH OBSERVABILITY

Now create:

`utils/instrumentedGemini.js`

```js
// utils/instrumentedGemini.js

import ai from "./geminiClient.js";
import { logInfo, logWarn, logError, truncate } from "./logger.js";

export async function geminiCall(operation, payload) {
  logInfo(`[${operation}] Request`, {
    model: payload.model,
    contents: truncate(payload.contents, 200)
  });

  const start = Date.now();

  try {
    const response = await ai.models.generateContent(payload);

    const latency = Date.now() - start;

    logInfo(`[${operation}] Latency (ms)`, latency);

    // Log token usage if provided
    if (response?.usageMetadata) {
      logInfo(`[${operation}] Tokens`, response.usageMetadata);
    } else {
      logWarn(`[${operation}] No token metadata returned`);
    }

    // Log model version
    if (response?.modelVersion) {
      logInfo(`[${operation}] Model Version`, response.modelVersion);
    }

    // Log the truncated response output
    logInfo(`[${operation}] Output`, truncate(response.text));

    // Return response to caller
    return response;
  } catch (err) {
    logError(`[${operation}] FAILED`, err);

    // Classify errors
    const msg = err?.message?.toLowerCase() || "";

    if (msg.includes("timeout")) logWarn("Failure Type: TIMEOUT");
    else if (msg.includes("429")) logWarn("Failure Type: RATE LIMIT");
    else if (msg.includes("network")) logWarn("Failure Type: NETWORK FAILURE");
    else logWarn("Failure Type: UNKNOWN / MODEL");

    throw err; // Re-throw after logging
  }
}
```

Now every Gemini call is fully logged, classified, measured.

---

# PART 3 — UPDATED example script

Now update your `10a.observability-basic.js` to use the **instrumented wrapper**:

```js
// src/10b.observability-advanced.js

import { geminiCall } from "../utils/instrumentedGemini.js";
import { logStart, logEnd } from "../utils/logger.js";

async function main() {
  logStart("Describe Model - Advanced Observability");

  const prompt = "Explain Generative AI in one line.";

  const response = await geminiCall("DescribeModel", {
    model: "gemini-2.5-flash",
    contents: prompt
  });

  console.log("\nFinal Output:", response.text);

  logEnd("Describe Model - Advanced Observability");
}

await main();
```

Now your output will look like:

```
=== START: Describe Model - Advanced Observability ===
[INFO] [DescribeModel] Request
{ model: 'gemini-2.5-flash', contents: 'Explain Generative AI in one line.' }
[INFO] [DescribeModel] Latency (ms): 5339
[INFO] [DescribeModel] Tokens:
{ promptTokenCount: 9, ... }
[INFO] [DescribeModel] Model Version:
'gemini-2.5-flash'
[INFO] [DescribeModel] Output:
"Generative AI creates new content..."
=== END:   Describe Model - Advanced Observability ===
```

---

# PART 4 — Sanity tests you can run

Run these:

Test 1 — Normal:

```js
"Explain Generative AI in one line."
```

Test 2 — Large content:

```js
"Explain how Transformers work in 10 sections."
```

Test 3 — Force error:
Turn off internet or add wrong model name:

```js
model: "gemini-2.5-flash-XYZ"
```

Output will classify failure.

Test 4 — Long output truncation test:

```js
"Write a 1000-word essay on quantum computing."
```

Truncation should kick in.

---

# PART 5 — Want even deeper observability?

add:

* **Middleware chaining (like Express)** for Gemini calls
* **OpenTelemetry traces** (full distributed tracing)
* **Performance histogram** for requests
* **File-based log storage**
* **Agent memory + observability** combined logs
* **Automatic cost estimation** based on token usage

