# **Topic 2: Understanding Model Parameters & Config in Gemini SDK**

* Followed: 
    * https://ai.google.dev/gemini-api/docs/text-generation
    
---

## **1. Goal**

Learn how to:

* Control model behavior using `config`
* Understand `temperature`, `thinking`, and `system instructions`
* Build your own **“Describe Model”** script
* Prepare for **streaming** and **multi-turn chat** features next

---

## **2. Concept: How LLMs Generate Text**

When you give a prompt like

> “Explain how AI works”

the model doesn’t return a fixed answer.
It predicts the next most probable token (word/piece) repeatedly until the answer ends.
That’s why we have **control parameters** that affect:

* Randomness
* Length
* Tone
* Personality

---

## **3. Key Parameters (Used inside `config`)**

| Parameter             | Meaning                                                                   | Example                                                     |
| --------------------- | ------------------------------------------------------------------------- | ----------------------------------------------------------- |
| **temperature**       | Controls randomness. Lower = deterministic, Higher = creative             | `temperature: 0.1` (precise), `temperature: 1.0` (creative) |
| **maxOutputTokens**   | Limits maximum words/tokens in output                                     | `maxOutputTokens: 200`                                      |
| **systemInstruction** | Sets model “persona” or role                                              | `"You are a helpful coding assistant."`                     |
| **thinkingConfig**    | Controls “thinking budget” (internal reasoning). Setting to 0 disables it | `{ thinkingBudget: 0 }`                                     |
| **responseMimeType**  | Set structured output format (e.g., JSON)                                 | `"application/json"`                                        |
| **topP**              | Sampling strategy (focuses on most probable words). Lower = safer         | `topP: 0.9`                                                 |

---

## **4. Code Example: Describe Model (Topic 2 Starter)**

**File:** `src/02.describe-model.js`

```js
// -------------------------------------------------------------
// GOAL: Understand Gemini model parameters and configuration
// -------------------------------------------------------------
// This script demonstrates how to:
// - Control model behavior using config
// - Use system instructions
// - Adjust temperature (randomness)
// - Disable or enable "thinking"
// -------------------------------------------------------------

import ai from "../utils/geminiClient.js";

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "Describe yourself briefly and list three capabilities.",
    config: {
      temperature: 0.2,  // lower randomness → more factual answers
      maxOutputTokens: 150, // limit output length
      systemInstruction: "You are an expert AI tutor explaining clearly.",
      thinkingConfig: {
        thinkingBudget: 0 // disable thinking for faster response
      }
    }
  });

  console.log("Response:\n");
  console.log(response.text);
}

await main();
```

---

### **Expected Output**

```
Response:

I am Gemini 2.5 Flash, a large language model by Google.
My key capabilities:
1. Text and code generation
2. Understanding images and documents
3. Providing structured JSON or conversational responses
```

---

## **5. Code Deep Dive**

```js
config: {
  temperature: 0.2, 
  maxOutputTokens: 150,
  systemInstruction: "You are an expert AI tutor explaining clearly.",
  thinkingConfig: { thinkingBudget: 0 }
}
```

**Explanation:**

* `temperature`: Lower value ensures stable, factual output.
* `maxOutputTokens`: Controls how long the answer can be.
* `systemInstruction`: Sets tone/personality for all responses.
* `thinkingConfig`: Gemini 2.5 models “think” internally; setting this to 0 disables that reasoning to speed up short answers.

---

## **6. Variation Exercise**

Try different configurations to observe the change.

### **Example 1: High Temperature**

```js
temperature: 0.9
```

→ You’ll get more creative, varied wording every run.

### **Example 2: Add Persona**

```js
systemInstruction: "You are a humorous assistant."
```

→ You’ll see playful responses.

### **Example 3: Longer Answer**

```js
maxOutputTokens: 300
```

→ You’ll see detailed paragraphs.

---

## **7. Notes for your `Topics` Folder**

**File:**
`/Topics OR Concepts/topic2/01.model-parameters.md`

**Content:**

```
# Topic 2: Model Parameters and Configuration

- temperature → randomness control
- maxOutputTokens → output length
- systemInstruction → defines model's role
- thinkingConfig → control reasoning
- topP → sampling method
- responseMimeType → structured outputs (e.g., JSON)

Practical:
Use these configs inside ai.models.generateContent() to guide output style.
```

---

## **8. Next Step Preview**

## ✅ WHY these fields must be TOP-LEVEL, not inside config

**Gemini SDK internally uses two different layers:**

#### 1. Layer 1 — Request Body (Top-Level Fields)

* These fields actually go directly to the API:

```md
model
contents
systemInstruction
tools
responseMimeType
responseJsonSchema
```

These define WHAT the model should do.
The API expects them directly at the top:

```json
{
  "model": "...",
  "system_instruction": "...",
  "contents": [...],
  "response_mime_type": "...",
  "response_json_schema": {...}
}
```
IMPORTANT:
The Gemini backend will not read these fields if they are nested.

#### 2. Layer 2 — Generation Config (Inside config)

**This only controls HOW the model generates:**

```md
temperature
topP
topK
maxOutputTokens
safetySettings
thinkingConfig
```

These fields are NOT part of the main request body.

They become:
```json
"generation_config": {
  "temperature": 0.1,
  "maxOutputTokens": 100
}
```
This means config is ONLY for generation tuning, nothing else.

## So what goes where?
| Goes TOP-LEVEL       | Goes inside `config:` |
| -------------------- | --------------------- |
| `systemInstruction`  | `temperature`         |
| `responseJsonSchema` | `maxOutputTokens`     |
| `responseMimeType`   | `thinkingConfig`      |
| `tools`              | `topK`, `topP`        |
| `model`              | `stopSequences`       |
| `contents`           |                       |
