# **Model Parameters in Gemini SDK — Deep Engineering Notes**

---

# **0. Purpose of This File**

This file teaches you:

1. Every parameter Gemini accepts
2. Why each parameter exists
3. How it affects reasoning, sampling, creativity, structure, and safety
4. Which fields belong at **top-level** vs **inside `generation_config`**
5. How these controls affect agent reliability
6. Misuse patterns, edge-cases, and production rules

This is required before you start:

* Structured output
* Tool calling
* Multi-turn chat
* Agents
* Observability

This file explains **the mental model** behind LLM behavior control.

---

# **1. Two-Layer Architecture of the Gemini API Request**

You must understand Gemini’s API structure.

Gemini has **two separate control layers**:

---

## **Layer 1 — TOP-LEVEL Request Body (WHAT to do)**

These fields define:

* Purpose
* Task
* Format
* Tools
* Schema
* Role

This is the “instruction layer”.

These fields go **directly to Gemini’s core engine**:

| Top-Level Field                          | Meaning                          |
| ---------------------------------------- | -------------------------------- |
| `model`                                  | Which Gemini version to use      |
| `contents`                               | User input messages              |
| `systemInstruction`                      | Role / persona / rules           |
| `tools`                                  | Allowed tool definitions         |
| `responseMimeType`                       | Output format (e.g., JSON, text) |
| `responseSchema` or `responseJsonSchema` | Strict output structure          |
| `safetySettings`                         | Override safety categories       |

**These must NEVER be placed inside `config`.**

Gemini backend ignores them if nested.

---

## **Layer 2 — generation_config (HOW to generate)**

This controls:

* Randomness
* Creativity
* Token limits
* Internal reasoning budget
* Sampling strategy

This is the “sampling layer”.

Only these belong inside `config`:

| Inside `config`    | Meaning                       |
| ------------------ | ----------------------------- |
| `temperature`      | randomness                    |
| `topP`             | nucleus sampling              |
| `topK`             | restrict top K tokens         |
| `maxOutputTokens`  | output limit                  |
| `stopSequences`    | stop generation on patterns   |
| `candidateCount`   | number of independent outputs |
| `thinkingConfig`   | internal reasoning budget     |
| `responseLogprobs` | return token probabilities    |

---

## **The Rule (Memorize this):**

**Top-level = Task Specification**
**Config = Sampling Specification**

If you violate this separation, your model responses will be inconsistent.

---

# **2. The Core Parameters (Full Deep Explanation)**

Below are the most important parameters you will use in every agent or LLM system.

---

## **2.1 temperature** (0.0–2.0)

Controls randomness during token selection.

**Low temperature (0.0–0.3):**

* Deterministic
* Consistent outputs
* Ideal for:

  * structured output
  * schema-based responses
  * factual answers
  * API calls
  * agents

**Medium temperature (0.4–0.7):**

* Balanced creativity
* Good for chat, docs, general assistance

**High temperature (0.8–1.5):**

* Creative writing
* Brainstorming
* Poetry, stories

**Extreme (1.5–2.0):**

* Chaotic
* Unstable
* Useful only for creative idea explosion

**Engineering rule:**

For agents, schema enforcement, and tool-calling,
use **temperature: 0.0–0.3** for reliability.

---

## **2.2 topP** (0.1–1.0)

Nucleus sampling.

At each token:

Gemini collects the top tokens until their combined probability ≥ `topP`.

**topP = 1.0:**
Take all tokens (natural distribution).

**topP = 0.8:**
Only take tokens that contribute to 80% of probability.

**Function:**

Reduces randomness without killing creativity like temperature does.

**Engineering rule:**

* For stable agents → `topP: 0.8–0.9`
* For creative outputs → `topP: 0.95–1.0`

---

## **2.3 topK**

Restrict sampling to top K candidates only.

**topK = 1:**
Fully deterministic, picks the #1 token every time.

**topK = 40:**
Good balance for chatbots.

**topK = 200+:**
Broad diversity.

**Engineering rule:**

If you want maximum reliability:
`topK: 1–20`

Never high for structured output workflows.

---

## **2.4 candidateCount (1–8)**

Gemini internally generates multiple independent answers.

Example:

```
candidateCount: 4
```

Gemini creates four different continuations of the prompt.

Useful for:

* Ranking answers
* Reducing hallucination
* Generating multiple creative variations
* Code solutions
* Agent self-evaluation

**Pro Tip:**
Using candidateCount is more powerful than temperature when you want diversity without losing structure.

---

## **2.5 maxOutputTokens**

Maximum tokens Gemini is allowed to generate.

Must be set correctly because:

Too low → truncated answers
Too high → increased cost

**Engineering rule:**

* Short answers → 100–300
* Full explanations → 500–1000
* High-detail reasoning → 2000–3000
* Agents → set high (2000+) because tool output may require longer reasoning

---

## **2.6 stopSequences**

Gemini stops generating when it matches a token sequence.

Useful for:

* Markdown termination
* Tag-based generation
* Template generation
* Multi-part answers

Example:

```
stopSequences: ["</json>"]
```

---

## **2.7 thinkingConfig** (Gemini 2.5+)

Controls internal hidden chain-of-thought reasoning.

```
thinkingConfig: {
  thinkingBudget: number
}
```

**thinkingBudget = 0**

* Very fast
* Shallow reasoning
* Best for simple Q&A and structured JSON

**thinkingBudget > 0**

* Deeper reasoning
* More accuracy in multi-step logic
* Slightly slower

---

# **3. Model-Level Parameters (Top-Level Only)**

These define the behavior and structure of output.

---

## **3.1 systemInstruction**

Defines:

* role
* writing style
* constraints
* allowed behavior
* format rules

Example:

```
systemInstruction: 
"You are an expert assistant that replies only in valid JSON."
```

This affects:

* hidden chain-of-thought
* token selection
* prioritization
* formatting
* tool-calling behavior

---

## **3.2 responseMimeType**

Defines output type.

Most important:

### **1. "application/json"**

Turns Gemini into a JSON-output machine.

Prevents:

* extra text
* explanations
* formatting errors

### **2. "text/plain"**

Default.

### **3. "application/x.enum"**

Single value from allowed list.

---

## **3.3 responseSchema / responseJsonSchema**

Strict schema Gemini must follow.

Example:

```
responseSchema: {
  type: "object",
  properties: {
    answer: { type: "string" },
    confidence: { type: "number" }
  },
  required: ["answer"]
}
```

Schema transforms Gemini into:

* deterministic
* non-hallucinating
* production-safe

Gemini will:

* prune any invalid token
* re-generate until schema is matched
* prefer valid structure over creativity

This is the foundation for agent development.

---

## **3.4 tools**

Defines all callable tools.

We will cover this in **Topic 4**.

---

# **4. Full Parameter Classification Table (Deep)**

| Type                            | Fields                                                                                                        | Description                             |
| ------------------------------- | ------------------------------------------------------------------------------------------------------------- | --------------------------------------- |
| **Top-Level → Task Definition** | `model`, `contents`, `systemInstruction`, `tools`, `responseMimeType`, `responseJsonSchema`, `safetySettings` | Defines what the request is doing       |
| **Config → Sampling**           | `temperature`, `topP`, `topK`, `maxOutputTokens`, `candidateCount`, `stopSequences`, `thinkingConfig`         | Controls how the model generates tokens |
| **Advanced**                    | `toolConfig`, `responseLogprobs`, `structuredOutputs`                                                         | Optional low-level controls             |

---

# **5. How Parameters Interact (Critical Section)**

Parameters are **not independent**.

### 5.1 temperature × topP × topK

These three together define sampling shape.

Low temperature + low topP = deterministic.
High temperature + high topP = creative chaos.

---

### 5.2 Schema × temperature

Schema always wins.

Even with temperature 0.9, if schema requires:

```
{ "city": string }
```

Output will still be deterministic.

Schema collapses high randomness into structured output.

---

### 5.3 thinkingConfig × candidateCount

Increasing both:

* increases depth
* increases accuracy
* lowers hallucination
* increases cost

This is ideal for mathematical agents or planning agents.

---

### 5.4 systemInstruction × tools

A strict system instruction like:

```
Only call a tool if necessary.
```

dramatically affects tool-selection logic.

---

# **6. Best Practices (Production)**

## 6.1 For deterministic agents

```
temperature: 0.0–0.2
topK: 1–20
topP: 0.8–0.9
responseMimeType: application/json
```

## 6.2 For structured pipelines

```
responseJsonSchema: { ... }
temperature: 0.0–0.2
thinkingBudget: 0
```

## 6.3 For reasoning-heavy tasks

```
temperature: 0.2–0.4
thinkingBudget: 50–200
candidateCount: 2–4
```

## 6.4 For creative writing

```
temperature: 0.8+
topP: 0.9+
candidateCount: 1
```

---

# **7. Common Mistakes (Avoid These)**

## Mistake 1

Placing schema or systemInstruction inside `config`.

Incorrect:

```
config: {
  systemInstruction: "..."
}
```

Correct:

```
systemInstruction: "..."
```

---

## Mistake 2

Not setting maxOutputTokens → truncated answers.

---

## Mistake 3

Using high temperature with schema → inconsistent formatting.

---

## Mistake 4

Overusing thinkingBudget for simple tasks → unnecessary cost.

---

# **8. Summary (Interview Ready)**

1. Parameters split into **task-level** and **generation-level**.
2. Task-level parameters define the purpose and structure.
3. Generation-level parameters define randomness and token sampling.
4. Schema overrides temperature and ensures reliable output.
5. System instruction changes overall model behavior.
6. thinkingConfig allows deeper internal reasoning.
7. Flash models need stricter constraints; Pro models handle reasoning better.

This is the full mental model of Gemini’s configuration system.

---
