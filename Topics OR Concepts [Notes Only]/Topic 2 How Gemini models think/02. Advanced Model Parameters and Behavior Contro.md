# Advanced Model Parameters and Behavior Control**

---

## **1. Goal**

Learn to:

* Fine-tune model *determinism* using `topP` and `topK`
* Use `responseMimeType` and `responseSchema` for structured outputs
* Control response *length* and *format*
* Log and analyze model metadata

---

## **2. Advanced Sampling Parameters**

LLMs predict the next token based on probabilities.
You can limit how “wide” the sampling distribution is by using **temperature**, **topP**, and **topK** together.

| Parameter       | What It Does                                                                           | Range  | Typical Value |
| --------------- | -------------------------------------------------------------------------------------- | ------ | ------------- |
| **temperature** | Adds randomness globally.                                                              | 0–1    | 0.2 – 0.8     |
| **topP**        | Nucleus sampling: keep the smallest set of tokens whose cumulative probability ≥ topP. | 0–1    | 0.9           |
| **topK**        | Limits sampling to top K most likely tokens.                                           | 1–1000 | 40            |

**Key insight:**
Use *either* `topP` *or* `topK` (rarely both) to bound randomness precisely.

---

### **Example: Compare topP and topK**

**File:** `src/02a.sampling-control.js`

```js
// Demonstrate how topP and topK affect variation.
import ai from "../utils/geminiClient.js";

const prompt = "Give one creative slogan for a coffee brand.";

async function run() {
  console.log("\n--- Using topP: 0.3 (very focused) ---");
  let res1 = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: prompt,
    config: { temperature: 0.7, topP: 0.3 }
  });
  console.log(res1.text);

  console.log("\n--- Using topK: 100 (wider choices) ---");
  let res2 = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: prompt,
    config: { temperature: 0.7, topK: 100 }
  });
  console.log(res2.text);
}
await run();
```

**Observation:**
Low topP → focused, predictable output.
High topK → more diverse and creative text.

---

## **3. Response Formatting Parameters**

| Parameter            | Description                                                    | Example              |
| -------------------- | -------------------------------------------------------------- | -------------------- |
| **responseMimeType** | Controls format of output                                      | `"application/json"` |
| **responseSchema**   | Ensures model obeys JSON schema (used with `responseMimeType`) | see below            |
| **candidateCount**   | Ask model to produce multiple alternative responses            | `candidateCount: 3`  |
| **stopSequences**    | Stop generation when one of the sequences is reached           | `["###"]`            |

---

### **Example: Multiple Candidates**

**File:** `src/02b.multiple-candidates.js`

```js
import ai from "../utils/geminiClient.js";

async function main() {
  const prompt = "Write one line describing AI for high school students.";
  const res = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: prompt,
    config: {
      candidateCount: 3,  // Ask for 3 variations
      temperature: 0.8
    }
  });

  console.log("Received", res.candidates.length, "candidates:\n");
  res.candidates.forEach((c, i) => {
    console.log(`Option ${i + 1}:`, c.content.parts[0].text);
  });
}
await main();
```

**Use case:** choose or rank responses automatically.

---

## **4. Structured JSON Outputs (Brief Intro)**

`responseMimeType: "application/json"`
lets you ask the model to output **machine-readable JSON** directly.
You’ll use this fully in *Topic 2.2*, but here’s a minimal teaser:

```js
config: {
  responseMimeType: "application/json",
  responseSchema: {
    type: "object",
    properties: {
      title: { type: "string" },
      summary: { type: "string" }
    },
    required: ["title", "summary"]
  }
}
```

This guarantees the model responds with something like:

```json
{ "title": "AI in Education", "summary": "AI helps personalize learning..." }
```

---

## **5. Thinking Configuration (for Gemini 2.5)**

Gemini 2.5 introduces **internal reasoning** or “thinking”.
You can manage its computational budget:

| Field                                  | Purpose                                  | Example                           |
| -------------------------------------- | ---------------------------------------- | --------------------------------- |
| `thinkingConfig.thinkingBudget`        | Limits reasoning effort; 0 disables      | `{ thinkingBudget: 0 }`           |
| `thinkingConfig.reasoningBudgetTokens` | (Advanced) cap internal reasoning tokens | `{ reasoningBudgetTokens: 1024 }` |

**Trade-off:** more thinking → higher latency, but often more accurate.

---

## **6. Debugging and Observability**

Each `generateContent` call returns metadata.
You can log tokens, safety results, or candidate info:

```js
console.log("Model:", res.modelVersion);
console.log("Tokens used:", res.usageMetadata);
```

For large systems you’ll later integrate **Observability** (Topic 6).

---

## **7. Notes**

**File:** `/Topics OR Concepts/topic2/02.advanced-config.md`

```
# Topic 2.1 – Advanced Configuration

- topP / topK control sampling diversity.
- candidateCount gives multiple completions.
- stopSequences let you terminate generation early.
- responseMimeType and responseSchema enforce output structure.
- thinkingConfig manages internal reasoning budget.
- usageMetadata reveals token cost and diagnostics.
```

---

## **8. Practice Tasks**

1. Compare three runs:

   * `temperature: 0.2`
   * `temperature: 0.9`
   * `temperature: 0.9, topP: 0.3`
     Observe tone differences.

2. Use `candidateCount: 3` and write code to automatically pick the *shortest* response.

3. Add `stopSequences: ["END"]` to a prompt like
   `"List fruits, end each with END"`, and verify early stop.

---
